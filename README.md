<div align="center">

# <b>MonoGaussianAvatar</b>: Monocular Gaussian Point-based Head Avatar

[Yufan Chen](https://yufan1012.github.io)<sup>1</sup>, [Lizhen Wang](https://lizhenwangt.github.io/)<sup>2</sup>, [Qijing Li](https://www.liuyebin.com/student.html)<sup>2</sup>, [Hongjiang Xiao](https://www.semanticscholar.org/author/Hongjiang-Xiao/2747760)<sup>3</sup>, [Shengping Zhang](http://homepage.hit.edu.cn/zhangshengping)<sup>1</sup>, [Hongxun Yao](http://homepage.hit.edu.cn/yaohongxun)<sup>1</sup>, [Yebin Liu](https://www.liuyebin.com)<sup>2</sup>

<sup>1</sup>Harbin Institute of Technology <sup>2</sup>Tsinghua Univserity <sup>3</sup>Communication University of China

### [Projectpage](https://yufan1012.github.io/MonoGaussianAvatar) · [Paper](https://arxiv.org/pdf/2312.02155.pdf) · [Video]([https://youtu.be/TBIekcqt0j0](https://www.youtube.com/embed/3UvBkyPc-oc))

</div>

<img src="https://yufan1012.github.io/assets/MonoGaussianAvatar/images/teaser.png">

***Abstract**: The ability to animate photo-realistic head avatars reconstructed from monocular portrait video sequences represents a crucial step in bridging the gap between the virtual and real worlds. Recent advancements in head avatar techniques, including explicit 3D morphable meshes (3DMM), point clouds, and neural implicit representation have been exploited for this ongoing research. However, 3DMM-based methods are constrained by their fixed topologies, point-based approaches suffer from a heavy training burden due to the extensive quantity of points involved, and the last ones suffer from limitations in deformation flexibility and rendering efficiency. In response to these challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head Avatar), a novel approach that harnesses 3D Gaussian point representation coupled with a Gaussian deformation field to learn explicit head avatars from monocular portrait videos. We define our head avatars with Gaussian points characterized by adaptable shapes, enabling flexible topology. These points exhibit movement with a Gaussian deformation field in alignment with the target pose and expression of a person, facilitating efficient deformation. Additionally, the Gaussian points have controllable shape, size, color, and opacity combined with Gaussian splatting, allowing for efficient training and rendering. Experiments demonstrate the superior performance of our method, which achieves state-of-the-art results among previous methods.*

Code is coming soon.

## Animation Results
### Self-drive

https://github.com/ShunyuanZheng/GPS-Gaussian/assets/33752042/36d06407-fadc-485a-864b-961fbd4d4b60

### Cross-identity Reenactment

https://github.com/ShunyuanZheng/GPS-Gaussian/assets/33752042/d392673c-13cd-442d-aa94-6629c9edfb3c

https://github.com/ShunyuanZheng/GPS-Gaussian/assets/33752042/371171ca-46a9-427b-9549-9d65fc4b135d
